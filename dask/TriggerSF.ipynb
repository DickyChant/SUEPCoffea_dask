{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89478bce-c68d-4738-b62d-b288e49503ba",
   "metadata": {},
   "source": [
    "# Dask through Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66b82e-fde3-4595-b4f9-24e25e6ae57d",
   "metadata": {},
   "source": [
    "This notebook runs a simple study on SUEP data using Dask, creating an output coffea file that can be analyzed in Dask_analysis.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c07901-136b-4e4e-9146-33a52a8d671e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/submit/freerc/miniforge3/envs/dask/lib/python3.9/site-packages/coffea/util.py:154: FutureWarning: In coffea version v0.8.0 (target date: 31 Dec 2022), this will be an error.\n",
      "(Set coffea.deprecations_as_errors = True to get a stack trace now.)\n",
      "ImportError: coffea.hist is deprecated\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/work/submit/freerc/miniforge3/envs/dask/lib/python3.9/site-packages/dask_jobqueue/core.py:20: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import tmpfile\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import coffea\n",
    "coffea.deprecations_as_errors = False #Get rid of warning for now\n",
    "from coffea import hist, processor\n",
    "from hist import Hist\n",
    "import matplotlib\n",
    "\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "from dask.distributed import performance_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841c2aa-987c-4a20-8911-a8923f3d36f1",
   "metadata": {},
   "source": [
    "We set up a coffea ABC Processor to analyze the ROOT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768afee5-9257-4b71-a695-40c32101a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "\n",
    "class Simple_Process(processor.ProcessorABC):\n",
    "    def __init__(self, isMC: int, era: int, sample: str) -> None:\n",
    "        self.gensumweight = 1.0\n",
    "        self.era = era\n",
    "        self.isMC = isMC\n",
    "        self.sample = sample\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator(\n",
    "            {\n",
    "                \"ht_reco\": hist.Hist(\n",
    "                    \"Events\",\n",
    "                    hist.Cat(\"dataset\", \"Dataset\"),\n",
    "                    hist.Bin(\"ht_reco\", r\"$H_T$ [GeV]\", 50,0,2500),\n",
    "                ),\n",
    "                \"ht_reco_triggered\": hist.Hist(\n",
    "                    \"Events\",\n",
    "                    hist.Cat(\"dataset\", \"Dataset\"),\n",
    "                    hist.Bin(\"ht_reco_triggered\", r\"$H_T$ [GeV]\",  50,0,2500),\n",
    "                ),\n",
    "                \"nmuons\": hist.Hist(\n",
    "                    \"Events\",\n",
    "                    hist.Cat(\"dataset\", \"Dataset\"),\n",
    "                    hist.Bin(\"nmuons\", r\"$N_{muons}$\", 30, 0, 30),\n",
    "                ),\n",
    "                \"muon_pt\": hist.Hist(\n",
    "                    \"Events\",\n",
    "                    hist.Cat(\"dataset\", \"Dataset\"),\n",
    "                    hist.Bin(\"muon_pt\", r\"$Muon p_{T}$ [GeV]\", 10, 0, 200),\n",
    "                ),\n",
    "                \"muon_pt_triggered\": hist.Hist(\n",
    "                    \"Events\",\n",
    "                    hist.Cat(\"dataset\", \"Dataset\"),\n",
    "                    hist.Bin(\"muon_pt_triggered\", r\"$Muon p_{T}$ [GeV]\", 10, 0, 200),\n",
    "                ),\n",
    "                \"MET\": hist.Hist(\n",
    "                    \"Events\",\n",
    "                    hist.Cat(\"dataset\", \"Dataset\"),\n",
    "                    hist.Bin(\"MET\", r\"$p_{T}^{miss}$ [GeV]\", 50, 0, 200),\n",
    "                ),\n",
    "                \"sumw\": processor.defaultdict_accumulator(float),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, events):\n",
    "        output = self.accumulator\n",
    "        dataset = events.metadata['dataset']\n",
    "        #if \"Muon\" not in dataset:\n",
    "        try:\n",
    "            self.gensumweight = ak.sum(events.genWeight)\n",
    "            output[\"sumw\"][dataset] += ak.sum(events.genWeight)\n",
    "        except:\n",
    "            output[\"sumw\"][dataset] = 1.0\n",
    "        #output[\"sumw\"][dataset] = 1.0\n",
    "        \n",
    "        muons = ak.zip({\n",
    "            \"pt\": events.Muon.pt,\n",
    "            \"eta\": events.Muon.eta,\n",
    "            \"phi\": events.Muon.phi,\n",
    "            \"mass\": events.Muon.mass,\n",
    "            \"mediumId\": events.Muon.mediumId\n",
    "        }, with_name=\"Momentum4D\") \n",
    "        muon_triggered = muons[events.HLT.IsoMu24 == 1]\n",
    "        muon_cut = (events.Muon.pt > 10) & \\\n",
    "            (abs(events.Muon.eta) <= 2.4) & \\\n",
    "            (events.Muon.mediumId == 1) \n",
    "        muons = muons[muon_cut]\n",
    "        muon_triggered_cut = (muon_triggered.pt > 10) & \\\n",
    "            (abs(muon_triggered.eta) <= 2.4) & \\\n",
    "            (muon_triggered.mediumId == 1) \n",
    "        muon_triggered = muon_triggered[muon_triggered_cut]\n",
    "        \n",
    "        Ak4Jets = ak.zip({\n",
    "            \"pt\": events.Jet.pt,\n",
    "            \"eta\": events.Jet.eta,\n",
    "            \"phi\": events.Jet.phi,\n",
    "            \"mass\": events.Jet.mass,\n",
    "        }, with_name=\"Momentum4D\")\n",
    "        #Ak4Jets = Ak4Jets[events.HLT.Mu45_eta2p1 == 1]#for 2016\n",
    "        \n",
    "        Jets_triggered = Ak4Jets[(events.HLT.PFHT1050 == 1) & (events.HLT.Mu50 == 1)]\n",
    "        Ak4Jets = Ak4Jets[events.HLT.Mu50 == 1]\n",
    "        Ak4JetCut = (Ak4Jets.pt > 30) & (abs(Ak4Jets.eta)<4.7)\n",
    "        Ak4Jets = Ak4Jets[Ak4JetCut]\n",
    "        Jets_triggeredCut = (Jets_triggered.pt > 30) & (abs(Jets_triggered.eta)<4.7)\n",
    "        Jets_triggered = Jets_triggered[Jets_triggeredCut]\n",
    "                \n",
    "        # fill out hists\n",
    "\n",
    "        output['ht_reco'].fill(ht_reco=ak.sum(Ak4Jets.pt,axis=-1), dataset=dataset)\n",
    "        jet_trig = ak.to_numpy(ak.sum(Jets_triggered.pt,axis=-1),allow_missing=True)\n",
    "        output['ht_reco_triggered'].fill(ht_reco_triggered=jet_trig, dataset=dataset)\n",
    "        \n",
    "        output['nmuons'].fill(nmuons=ak.num(muons, axis=-1), dataset=dataset)\n",
    "        muons = muons[ak.num(muons, axis=-1)>0]\n",
    "        output['muon_pt'].fill(muon_pt=ak.max(muons.pt, axis=-1), dataset=dataset)\n",
    "        muon_trig = ak.to_numpy(ak.max(muon_triggered.pt, axis=-1),allow_missing=True)\n",
    "        output['muon_pt_triggered'].fill(muon_pt_triggered=muon_trig, dataset=dataset)\n",
    "        output['MET'].fill(MET=events.MET.pt, dataset=dataset)\n",
    "                \n",
    "        return output\n",
    "        \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ae7fe0-0bda-471a-b79b-cc6e25849edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_port(port):\n",
    "    import socket\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    try:\n",
    "        sock.bind((\"0.0.0.0\", port))\n",
    "        available = True\n",
    "    except:\n",
    "        available = False\n",
    "    sock.close()\n",
    "    return available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f4b491-1ea5-4798-8186-8e184177ae20",
   "metadata": {},
   "source": [
    "The following section defines additional parts of the slurm Dask job. Here we source the bashrc to prepare Conda. We also pass in the x509 proxy. In order to share the proxy across the SubMIT machines you should move your proxy to your HOME directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a03cb39-ca96-48a9-ba57-c14bac3982f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_env = [\n",
    "     'export XRD_RUNFORKHANDLER=1',\n",
    "     'export XRD_STREAMTIMEOUT=10',\n",
    "     f'source {os.environ[\"HOME\"]}/.bashrc',\n",
    "     f'conda activate dask',\n",
    "     f'export X509_USER_PROXY={os.environ[\"HOME\"]}/x509up_u206148',\n",
    "     f'sleep $[ ( $RANDOM % 300 )  + 1 ]s'\n",
    "]\n",
    "\n",
    "extra_args=[\n",
    "     \"--output=dask_job_output_%j.out\",\n",
    "     \"--error=dask_job_output_%j.err\",\n",
    "     \"--partition=submit\",\n",
    "     \"--clusters=submit\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93adef70-3f14-4ace-bad2-6c5156f2d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_port       = 6820\n",
    "w_port       = 9765\n",
    "cores        = 1\n",
    "processes    = 1\n",
    "memory       = \"10 GB\"\n",
    "chunksize    = 100000\n",
    "maxchunks    = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c9ee4-6c47-4931-8844-b0b89b2353d8",
   "metadata": {},
   "source": [
    "The following sets up the processor and json file. If you want to change files you can simply modify the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20cbe8d-af0c-4a53-9fc6-0b592fdb1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load samples\n",
    "file = \"../filelist/trig_2018.txt\"\n",
    "samples = []\n",
    "with open(file, 'r') as stream:\n",
    "    for sample in stream.read().split('\\n'):\n",
    "        if '#' in sample: continue\n",
    "        if len(sample.split('/')) <= 1: continue\n",
    "        sample_name = sample.split(\"/\")[-1]\n",
    "        samples.append(sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7cfe42-357b-43cd-b0af-0c99ee1af8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file names\n",
    "samples_dict = {}\n",
    "for sample_name in samples:\n",
    "    \n",
    "    # ignores SUEP files since we want to use another one\n",
    "    if 'SUEP' in sample_name: continue\n",
    "    \n",
    "    input_list = \"/home/tier3/cmsprod/catalog/t2mit/nanosu/A01/{}/RawFiles.00\".format(sample_name)\n",
    "    \n",
    "    files = []\n",
    "    Raw_list = open(input_list, \"r\")\n",
    "    for i in Raw_list:\n",
    "        file = i.split(\" \")[0]\n",
    "        #file = file.replace(\"root://xrootd.cmsaf.mit.edu//store\",\"/mnt/T2_US_MIT/hadoop/cms/store\")\n",
    "        files.append(file)\n",
    "    \n",
    "    samples_dict[sample_name] = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2f91cd-9c86-47f7-b9bd-83638fc88e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SUEP files\n",
    "suepDir = \"/work/submit/freerc/Carlos/\"\n",
    "suep_dict = {}\n",
    "for sample in os.listdir(suepDir):\n",
    "    #if \"M125\" not in sample: continue\n",
    "    if \"ggHpythia_generic\" not in sample: continue\n",
    "    mass = sample.split(\"_\")[2]\n",
    "    suep_dict.update({mass: [suepDir + sample + \"/GEN/total.root\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f87b335-d8bb-4089-9ed1-cbd367286c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine QCD, data, and SUEP files\n",
    "process_dict = {}\n",
    "for key in list(samples_dict.keys()):\n",
    "    process_dict.update({key:samples_dict[key]})#Only take 10 files for now.\n",
    "#process_dict = process_dict | suep_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5ff65d-6088-48b8-b8c2-6e0eb0dfb05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e145bbf1-5282-4e86-8b5a-536d3d17af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross section\n",
    "xsections = {}\n",
    "for sample in list(process_dict.keys()):\n",
    "    xsection = 1.0\n",
    "    if 'QCD' in sample:\n",
    "        with open('../data/xsections_{}.json'.format('2018')) as file:\n",
    "            MC_xsecs = json.load(file)\n",
    "            try:\n",
    "                xsection *= MC_xsecs[sample][\"xsec\"]\n",
    "                xsection *= MC_xsecs[sample][\"kr\"]\n",
    "                xsection *= MC_xsecs[sample][\"br\"]\n",
    "            except:\n",
    "                print(\"WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\")\n",
    "                print(sample)\n",
    "        xsections.update({sample:xsection})\n",
    "    else:\n",
    "        xsections.update({sample:xsection})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578929b-790e-409d-bfdb-9befceae8eb1",
   "metadata": {},
   "source": [
    "The next section forms the Slurm Cluster. You can set up various parameters of the cluster here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ab5ca-0d06-4c30-96bb-24fa3915ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not check_port(n_port):\n",
    "    time.sleep(5)\n",
    "\n",
    "import socket\n",
    "cluster = SLURMCluster(\n",
    "        queue='all',\n",
    "        project=\"SUEP_Slurm\",\n",
    "        cores=4,\n",
    "        processes=processes,\n",
    "        memory=memory,\n",
    "        #retries=10,\n",
    "        walltime='00:30:00',\n",
    "        scheduler_options={\n",
    "              'port': n_port,\n",
    "              'dashboard_address': 8000,\n",
    "              'host': socket.gethostname()\n",
    "        },\n",
    "        job_extra=extra_args,\n",
    "        env_extra=slurm_env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298d20e-338a-489d-9e06-8f45d8fa3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(minimum=1, maximum=10)\n",
    "client = Client(cluster)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d3773f-9339-4922-8ca9-71eaff327f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processor_instance = Simple_Process(isMC=1, era='2018', sample='test')\n",
    "#output = processor.run_uproot_job(\n",
    "#    process_dict,\n",
    "#    treename='Events',\n",
    "#    processor_instance=processor_instance,\n",
    "#    executor=processor.futures_executor,\n",
    "#    executor_args={'workers': 30,\n",
    "#            'schema': processor.NanoAODSchema,\n",
    "#            'xrootdtimeout': 30,\n",
    "#        },\n",
    "#    chunksize=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb8d52-2382-48f9-9da3-e7ed4fbd0532",
   "metadata": {},
   "source": [
    "## Running the processor\n",
    "Now we will run the code with a performance report. This will analyze all of the input ROOT files and will store the histograms in output. Then we can analyze the output and make plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7312b5-e2da-4548-af9f-860e7646d63b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor_instance = Simple_Process(isMC=1, era='2018', sample='test')\n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    output = processor.run_uproot_job(process_dict,\n",
    "             treename='Events',\n",
    "             processor_instance=processor_instance,\n",
    "             executor=processor.dask_executor,\n",
    "             executor_args={\n",
    "                           'client': client,\n",
    "                           'skipbadfiles': True,\n",
    "                           'schema': processor.NanoAODSchema,\n",
    "                           'xrootdtimeout': 30,\n",
    "                           'retries': 10,\n",
    "                           },\n",
    "             chunksize=100000,\n",
    "             maxchunks=maxchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6e92f-4d9f-449f-98fe-34afd4ad7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.cancel(cluster)\n",
    "\n",
    "# just in case this isn't working as expected\n",
    "#coffea.util.save(output, \"unscaled_output.coffea\")\n",
    "\n",
    "# calculate normalization\n",
    "scales = {} \n",
    "for dataset in output[\"sumw\"]:\n",
    "    xsec = xsections[dataset]\n",
    "    scale = xsec / output[\"sumw\"][dataset]\n",
    "    if \"Muon\"  in dataset: scale = 1.0\n",
    "    scales.update({dataset: scale})\n",
    "\n",
    "    \n",
    "# apply normalization to all histograms\n",
    "for key in list(output.keys()):\n",
    "    if key.lower() == 'sumw': continue\n",
    "    print(key)\n",
    "    output[key].scale(scales, axis='dataset')\n",
    "\n",
    "coffea.util.save(output, \"output.coffea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658b324-d9b8-43ce-90c2-ab7fbe3326c3",
   "metadata": {},
   "source": [
    "### We can make some plots here too! But most of the analysis is in Dask_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351aea7-0ce9-4365-b67d-0de06589715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.hist import plot\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "\n",
    "#hep.cms.label(data=False, year='2018')\n",
    "hep.style.use(\"ROOT\")\n",
    "# {\"ALICE\" | \"ATLAS\" | \"CMS\" | \"LHCb1\" | \"LHCb2\"}\n",
    "hep.cms.label(data=False)\n",
    "\n",
    "_ = ax.set_yscale('log')\n",
    "plot.plot1d(output['ht_reco'][\"QCD_Pt*\"], ax=ax, clear=False, stack=True)\n",
    "_ = ax.set_xlim(0,2500)\n",
    "_ = ax.set_ylim(0, 10000000000)\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baeddee-951a-4b9a-9057-a8814e752b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "\n",
    "#hep.cms.label(data=False, year='2018')\n",
    "hep.style.use(\"ROOT\")\n",
    "# {\"ALICE\" | \"ATLAS\" | \"CMS\" | \"LHCb1\" | \"LHCb2\"}\n",
    "hep.cms.label(data=False)\n",
    "\n",
    "_ = ax.set_yscale('log')\n",
    "plot.plot1d(output['ht_reco_triggered'][\"*Muon*\"], ax=ax, clear=False, stack=True)\n",
    "_ = ax.set_xlim(0,2500)\n",
    "_ = ax.set_ylim(0, 10000000000)\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d217c-3c36-4230-882e-ef85fb0833ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trig = 0\n",
    "val = 0\n",
    "var_trig = 0\n",
    "var = 0\n",
    "i = 0\n",
    "for sample in output['ht_reco_triggered'].project('dataset').values():\n",
    "    if 'QCD_Pt' not in sample[0]: continue\n",
    "    bh_trig = output['ht_reco_triggered'][sample[0]].to_boost()\n",
    "    bh = output['ht_reco'][sample[0]].to_boost()\n",
    "    if i == 0:\n",
    "        bins_use = bh_trig.axes.edges[1][0]\n",
    "        val_trig = bh_trig.values()[0]\n",
    "        val= bh.values()[0]\n",
    "        var_trig = bh_trig.variances()[0]\n",
    "        var= bh.variances()[0]\n",
    "    else:\n",
    "        val_trig += bh_trig.values()[0]\n",
    "        val += bh.values()[0]\n",
    "        var_trig += bh_trig.variances()[0]\n",
    "        var += bh.variances()[0]\n",
    "    i += 1\n",
    "\n",
    "#val_trig = np.array([ x+y for x,y in zip(val_trig[0::2], val_trig[1::2]) ])\n",
    "#val = np.array([ x+y for x,y in zip(val[0::2], val[1::2]) ])\n",
    "#var_trig = np.array([ x+y for x,y in zip(var_trig[0::2], var_trig[1::2]) ])\n",
    "#var = np.array([ x+y for x,y in zip(var[0::2], var[1::2]) ])\n",
    "x = val_trig/val\n",
    "x2 = np.nan_to_num(x)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b202fa32-94f7-48ca-9ea7-7ac9c90c5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trig_data = 0\n",
    "val_data = 0\n",
    "var_trig_data = 0\n",
    "var_data = 0\n",
    "i = 0\n",
    "for sample in output['ht_reco_triggered'].project('dataset').values():\n",
    "    if 'Muon' not in sample[0]: continue\n",
    "    bh_trig_data = output['ht_reco_triggered'][sample[0]].to_boost()\n",
    "    bh_data = output['ht_reco'][sample[0]].to_boost()\n",
    "    if i == 0:\n",
    "        val_trig_data = bh_trig_data.values()[0]\n",
    "        val_data= bh_data.values()[0]\n",
    "        var_trig_data = bh_trig_data.variances()[0]\n",
    "        var_data= bh_data.variances()[0]\n",
    "    else:\n",
    "        val_trig_data += bh_trig_data.values()[0]\n",
    "        val_data += bh_data.values()[0]\n",
    "        var_trig_data+= bh_trig_data.variances()[0]\n",
    "        var_data += bh_data.variances()[0]\n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "#val_trig_data = np.array([ x+y for x,y in zip(val_trig_data[0::2], val_trig_data[1::2]) ])\n",
    "#val_data = np.array([ x+y for x,y in zip(val_data[0::2], val_data[1::2]) ])\n",
    "#var_trig_data = np.array([ x+y for x,y in zip(var_trig_data[0::2], var_trig_data[1::2]) ])\n",
    "#var_data = np.array([ x+y for x,y in zip(var_data[0::2], var_data[1::2]) ])\n",
    "x_data = val_trig_data/val_data\n",
    "x_data2 = np.nan_to_num(x_data)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d7137-21a8-4865-bb26-c171f876d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bins_use=bins_use[0::2]\n",
    "import hist\n",
    "MC_err = hist.intervals.ratio_uncertainty(num=val_trig,denom=val,uncertainty_type='efficiency')\n",
    "data_err = hist.intervals.ratio_uncertainty(num=val_trig_data,denom=val_data,uncertainty_type='efficiency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63559cc-f75e-4a6e-87c1-bf4da50a672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binc = np.array([ 0.5*(bins_use[i]+bins_use[i+1])for i in range(bins_use.shape[0]-1)])\n",
    "xerr = np.diff(bins_use)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc78d6-4d78-4a04-95b4-d48a6d563275",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "\n",
    "hep.cms.label(data=False, year='SingleMuon 2018')\n",
    "hep.style.use(\"ROOT\")\n",
    "# {\"ALICE\" | \"ATLAS\" | \"CMS\" | \"LHCb1\" | \"LHCb2\"}\n",
    "hep.cms.label(data=False)\n",
    "binc = np.array([ 0.5*(bins_use[i]+bins_use[i+1])for i in range(bins_use.shape[0]-1)])\n",
    "plt.errorbar(binc,x_data2, xerr=xerr,yerr=[data_err[0],data_err[1]],color=\"red\", fmt='o')\n",
    "plt.errorbar(binc,x2, xerr=xerr,yerr=[MC_err[0],MC_err[1]],color=\"black\", fmt='o')\n",
    "#plt.axvline(x=1200, color='gray',ls='--', lw=2)\n",
    "_ = ax.set_ylim(0.001, 130)\n",
    "_ = ax.set_ylabel(\"Efficiency\")\n",
    "_ = ax.set_xlabel(r\"AK4 $H_{T}$ [GeV]\")\n",
    "plt.rcParams['text.usetex'] = True\n",
    "labels = [r\"QCD MC\",r\"Data\"]\n",
    "leg = ax.legend(labels=labels)\n",
    "plt.axvline(x=1200, color='gray',ls='--', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26533ee5-25fa-480b-bae6-880bf3ddd73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z=np.nan_to_num(x_data2/x2)\n",
    "z=np.clip(z,0,15)\n",
    "z_up = np.nan_to_num((x_data2+data_err[0])/(x2-MC_err[1])) - z\n",
    "z_up = np.clip(z_up,0,15)\n",
    "z_down = z - np.nan_to_num((x_data2-data_err[1])/(x2+MC_err[0]))\n",
    "z_down = np.clip(z_down,0,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a5483-d913-4834-a6b4-29e4ca68ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z=np.clip(z,0,1.5)\n",
    "#z_up=np.clip(z_up,0,1.5)\n",
    "#_down=np.clip(z_down,0,1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85410be5-4aec-4c03-94ff-6a5577c2a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.subplots()\n",
    "\n",
    "hep.cms.label(data=False, year='SingleMuon 2018')\n",
    "hep.style.use(\"ROOT\")\n",
    "# {\"ALICE\" | \"ATLAS\" | \"CMS\" | \"LHCb1\" | \"LHCb2\"}\n",
    "hep.cms.label(data=False)\n",
    "plt.errorbar(binc,z, xerr=xerr,yerr=[z_up,z_down],color=\"red\", fmt='o')\n",
    "_ = ax.set_ylim(0.001, 1.5)\n",
    "_ = ax.set_ylabel(\"Scale Factor\")\n",
    "_ = ax.set_xlabel(r\"AK4 $H_{T}$ [GeV]\")\n",
    "plt.axvline(x=1200, color='gray',ls='--', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9169416-be4b-4150-882b-ce40e0426b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "outfile = uproot.recreate(\"trigSF_2018.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4f555-c7cf-411a-b320-4c5a046bf8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_var = np.maximum(z_up,z_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ec40e-27d6-42c3-b84a-ca6ef8e34499",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd26726-5a76-4450-9e07-594e46733ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boost_histogram as bh\n",
    "newhist = bh.Histogram(bh.axis.Variable(bins_use),storage=bh.storage.Weight())\n",
    "newhist[:] = np.stack([z, z_var], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae67918-30fd-4b67-b1f3-214a558e9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "newhist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b882991-0ef7-40d3-94e5-e8a37355ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outfile['efficiency_Data'] = x_data2\n",
    "#outfile['efficiency_MC'] = x_2\n",
    "outfile['TriggerSF'] = newhist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a46131-64ad-4d42-8cdd-14486520ae1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dask",
   "language": "python",
   "name": "dask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
