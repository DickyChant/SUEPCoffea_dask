{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ad632b",
   "metadata": {},
   "source": [
    "# Dask through Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0cdad0",
   "metadata": {},
   "source": [
    "This notebook runs a simple study on SUEP data using Dask, creating an output coffea file that can be analyzed in Dask_analysis.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a0c0b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/submit/lavezzo/miniforge3/envs/SUEP/lib/python3.9/site-packages/coffea/util.py:154: FutureWarning: In coffea version v0.8.0 (target date: 31 Dec 2022), this will be an error.\n",
      "(Set coffea.deprecations_as_errors = True to get a stack trace now.)\n",
      "ImportError: coffea.hist is deprecated\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import fastjet\n",
    "import awkward as ak\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "import coffea\n",
    "coffea.deprecations_as_errors = False #Get rid of warning for now\n",
    "from coffea import hist, processor\n",
    "from hist import Hist\n",
    "import matplotlib\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "from dask.distributed import performance_report\n",
    "from workflows.math_utils import *\n",
    "from workflows.SUEP_coffea import SUEP_cluster\n",
    "from plotting.plot_utils import *\n",
    "from dask_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5512360",
   "metadata": {},
   "source": [
    "We set up a coffea ABC Processor to analyze the ROOT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cce7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Process(processor.ProcessorABC):\n",
    "    def __init__(self, isMC: int, era: int, sample: str, scouting: int) -> None:\n",
    "        \n",
    "#         import os\n",
    "#         import sys\n",
    "#         import json\n",
    "#         import time\n",
    "#         import random\n",
    "#         import numpy as np\n",
    "#         import fastjet\n",
    "#         import awkward as ak\n",
    "#         import vector\n",
    "#         vector.register_awkward()\n",
    "\n",
    "#         sys.path.append(\"..\")\n",
    "#         from workflows.math_utils import sphericity\n",
    "        \n",
    "        self.gensumweight = 1.0\n",
    "        self.era = era\n",
    "        self.isMC = isMC\n",
    "        self.sample = sample\n",
    "        self.scouting = scouting\n",
    "\n",
    "        self._accumulator = processor.dict_accumulator(\n",
    "            {\n",
    "                \"sumw\": processor.defaultdict_accumulator(float),\n",
    "                \"total\": processor.defaultdict_accumulator(float),\n",
    "                \"cut1\": processor.defaultdict_accumulator(float),\n",
    "                \"cut2\": processor.defaultdict_accumulator(float),\n",
    "                \"cut3\": processor.defaultdict_accumulator(float),\n",
    "                \"cut4\": processor.defaultdict_accumulator(float),\n",
    "                \"cut5\": processor.defaultdict_accumulator(float)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    # this is modified from the normal SUEP_cluster one,\n",
    "    # since we need to count events after each selection\n",
    "    def eventSelection(self, events, output, dataset):\n",
    "        if self.scouting == 1:\n",
    "            Jets = ak.zip({\n",
    "                \"pt\": events.Jet.pt,\n",
    "                \"eta\": events.Jet.eta,\n",
    "                \"phi\": events.Jet.phi,\n",
    "                \"mass\": events.Jet.mass,\n",
    "            })\n",
    "        else:\n",
    "            Jets = ak.zip({\n",
    "                \"pt\": events.Jet.pt,\n",
    "                \"eta\": events.Jet.eta,\n",
    "                \"phi\": events.Jet.phi,\n",
    "                \"mass\": events.Jet.mass,\n",
    "                \"jetId\": events.Jet.jetId\n",
    "            })\n",
    "        jetCut = (Jets.pt > 30) & (abs(Jets.eta)<2.4)\n",
    "        ak4jets = Jets[jetCut]\n",
    "        ht = ak.sum(ak4jets.pt,axis=-1)\n",
    "        \n",
    "        # apply trigger selection\n",
    "        if self.scouting == 1:\n",
    "            events = events[(ht > 600)]\n",
    "            ak4jets = ak4jets[(ht > 600)]\n",
    "        else:\n",
    "            events = events[(ht > 1200)]\n",
    "            ak4jets = ak4jets[(ht > 1200)]\n",
    "            \n",
    "            output['cut1'][dataset] += len(events)\n",
    "            \n",
    "            if self.era == 2016:\n",
    "                trigger = (events.HLT.PFHT900 == 1)\n",
    "            else:\n",
    "                trigger = (events.HLT.PFHT1050 == 1)\n",
    "            \n",
    "            events = events[(trigger)]\n",
    "            ak4jets = ak4jets[(trigger)]\n",
    "            \n",
    "            output['cut2'][dataset] += len(events)\n",
    "            \n",
    "        return events, ak4jets\n",
    "\n",
    "    def process(self, events):\n",
    "        output = self.accumulator\n",
    "        dataset = events.metadata['dataset']\n",
    "        \n",
    "        os.system('sleep $[ ( $RANDOM % 100 )  + 1 ]s')\n",
    "        \n",
    "        # this needs to be here!\n",
    "        # FIXME: why isn't doing this in __init__ enough?\n",
    "        vector.register_awkward()\n",
    "\n",
    "        if self.isMC: self.gensumweight = ak.sum(events.genWeight)\n",
    "        \n",
    "        output[\"sumw\"][dataset] += ak.sum(events.genWeight)\n",
    "        output[\"total\"][dataset] += len(events)\n",
    "  \n",
    "        events, ak4jets = self.eventSelection(events, output, dataset)\n",
    "    \n",
    "        if len(events) == 0:\n",
    "            return output\n",
    "    \n",
    "        tracks, Cleaned_cands = SUEP_cluster.getTracks(self, events)\n",
    "        \n",
    "        # events = events[ak.num(tracks) > 0]\n",
    "        # tracks = tracks[ak.num(tracks) > 0]\n",
    "        indices = np.arange(0,len(tracks))\n",
    "        \n",
    "        if len(tracks) == 0:\n",
    "            return output\n",
    "        \n",
    "        ak_inclusive_jets, ak_inclusive_cluster = SUEP_cluster.FastJetReclustering(self, tracks, r=1.5, minPt=150)\n",
    "                \n",
    "        # remove events with at least 2 clusters (i.e. need at least SUEP and ISR jets for IRM)\n",
    "        clusterCut = (ak.num(ak_inclusive_jets, axis=1)>1)\n",
    "        ak_inclusive_cluster = ak_inclusive_cluster[clusterCut]\n",
    "        ak_inclusive_jets = ak_inclusive_jets[clusterCut]\n",
    "        tracks = tracks[clusterCut]\n",
    "        indices = indices[clusterCut]   \n",
    "        output['cut3'][dataset] += len(tracks)\n",
    "        \n",
    "        if len(tracks) == 0:\n",
    "            return output\n",
    "        \n",
    "        tracks, indices, topTwoJets = SUEP_cluster.getTopTwoJets(self, tracks, indices, ak_inclusive_jets, ak_inclusive_cluster)\n",
    "        SUEP_cand, ISR_cand, SUEP_cluster_tracks, ISR_cluster_tracks = topTwoJets\n",
    "        \n",
    "        # boost into frame of SUEP\n",
    "        boost_SUEP = ak.zip({\n",
    "            \"px\": SUEP_cand.px*-1,\n",
    "            \"py\": SUEP_cand.py*-1,\n",
    "            \"pz\": SUEP_cand.pz*-1,\n",
    "            \"mass\": SUEP_cand.mass\n",
    "        }, with_name=\"Momentum4D\")        \n",
    "        \n",
    "        # SUEP tracks for this method are defined to be the ones from the cluster\n",
    "        # that was picked to be the SUEP jet\n",
    "        SUEP_tracks_b = SUEP_cluster_tracks.boost_p4(boost_SUEP)        \n",
    "        \n",
    "        # SUEP jet variables\n",
    "        eigs = sphericity(SUEP_tracks_b,1.0) #Set r=1.0 for IRC safe\n",
    "        S1 = 1.5 * (eigs[:,1]+eigs[:,0])\n",
    "        nconst = ak.num(SUEP_tracks_b)\n",
    "        \n",
    "        tracks = tracks[(nconst > 80)]\n",
    "        S1 = S1[(nconst > 80)]\n",
    "        output['cut4'][dataset] += len(tracks)\n",
    "        \n",
    "        tracks = tracks[(S1 > 0.5)]\n",
    "        output['cut5'][dataset] += len(tracks)\n",
    "                \n",
    "        return output\n",
    "        \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad1f141",
   "metadata": {},
   "source": [
    "The following section defines additional parts of the slurm Dask job. Here we source the bashrc to prepare Conda. We also pass in the x509 proxy. In order to share the proxy across the SubMIT machines you should move your proxy to your HOME directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87fa2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm_env = [\n",
    "     'export XRD_RUNFORKHANDLER=1',\n",
    "     'export XRD_STREAMTIMEOUT=10',\n",
    "     f'source {os.environ[\"HOME\"]}/.bashrc',\n",
    "     f'conda activate SUEP',\n",
    "     f'export X509_USER_PROXY={os.environ[\"HOME\"]}/x509up_u210253',\n",
    "     'export PYTHONPATH=/home/submit/lavezzo/SUEP/SUEPCoffea_dask/:/home/submit/lavezzo/SUEP/SUEPCoffea_dask/workflows/:$PYTHONPATH'\n",
    "     # 'sleep $[ ( $RANDOM % 1000 )  + 1 ]s'\n",
    "]\n",
    "\n",
    "extra_args=[\n",
    "     \"--output=logs/dask_job_output_%j.out\",\n",
    "     \"--error=logs/dask_job_output_%j.err\",\n",
    "     \"--partition=submit\",\n",
    "     \"--clusters=submit\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c52011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_port       = 6820                   # might need to change this if re running\n",
    "w_port       = 9765\n",
    "cores        = 2\n",
    "processes    = 1\n",
    "memory       = \"10 GB\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6574a317",
   "metadata": {},
   "source": [
    "The following sets up the processor and json file. If you want to change files you can simply modify the json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "318bc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load samples\n",
    "file = \"../filelist/list_2018_SUEP_A01.txt\"\n",
    "samples = []\n",
    "with open(file, 'r') as stream:\n",
    "    for sample in stream.read().split('\\n'):\n",
    "        if '#' in sample: continue\n",
    "        if len(sample.split('/')) <= 1: continue\n",
    "        sample_name = sample.split(\"/\")[-1]\n",
    "        samples.append(sample_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f55edf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file names\n",
    "samples_dict = {}\n",
    "for sample_name in samples:\n",
    "    \n",
    "    input_list = \"/home/tier3/cmsprod/catalog/t2mit/nanosu/A01/{}/RawFiles.00\".format(sample_name)\n",
    "    \n",
    "    files = []\n",
    "    Raw_list = open(input_list, \"r\")\n",
    "    for i in Raw_list:\n",
    "        file = i.split(\" \")[0]\n",
    "        files.append(file)\n",
    "    \n",
    "    samples_dict[sample_name] = files\n",
    "process_dict = samples_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b2a0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n",
      "WARNING: I did not find the xsection for that MC sample. Check the dataset name and the relevant yaml file\n"
     ]
    }
   ],
   "source": [
    "# cross section\n",
    "xsections = {}\n",
    "for sample in list(process_dict.keys()):\n",
    "    xsection = getXSection(sample, '2018')\n",
    "    xsections.update({sample:xsection})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65087fe",
   "metadata": {},
   "source": [
    "The next section forms the Slurm Cluster. You can set up various parameters of the cluster here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb720bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/submit/lavezzo/miniforge3/envs/SUEP/lib/python3.9/site-packages/distributed/node.py:177: UserWarning: Port 8000 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 21170 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "while not check_port(n_port):\n",
    "    time.sleep(5)\n",
    "\n",
    "import socket\n",
    "cluster = SLURMCluster(\n",
    "        queue='all',\n",
    "        project=\"SUEP_Slurm\",\n",
    "        cores=cores,\n",
    "        processes=processes,\n",
    "        memory=memory,\n",
    "        #retries=10,\n",
    "        walltime='00:30:00',\n",
    "        scheduler_options={\n",
    "              'port': n_port,\n",
    "              'dashboard_address': 8000,\n",
    "              'host': socket.gethostname()\n",
    "        },\n",
    "    \n",
    "        job_extra=extra_args,\n",
    "        env_extra=slurm_env,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87cb2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://18.12.2.18:6820' processes=0 threads=0, memory=0 B>\n"
     ]
    }
   ],
   "source": [
    "cluster.adapt(minimum=1, maximum=250)\n",
    "client = Client(cluster)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c41a8d",
   "metadata": {},
   "source": [
    "## Running the processor\n",
    "Now we will run the code with a performance report. This will analyze all of the input ROOT files and will store the histograms in output. Then we can analyze the output and make plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d3801d8-b1dc-499b-aa79-7e3569344761",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(process_dict.keys())[0]\n",
    "debug_dict = {key: process_dict[key][:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1740c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#############################           ] | 72% Completed |  1min  8.1s"
     ]
    }
   ],
   "source": [
    "processor_instance = Simple_Process(isMC=1, era='2018', sample='test', scouting=0)\n",
    "with performance_report(filename=\"dask-report.html\"):\n",
    "    output = processor.run_uproot_job(debug_dict,\n",
    "             treename='Events',\n",
    "             processor_instance=processor_instance,\n",
    "             executor=processor.dask_executor,\n",
    "             executor_args={\n",
    "                           'client': client,\n",
    "                           'skipbadfiles': True,\n",
    "                           'schema': processor.NanoAODSchema,\n",
    "                           'xrootdtimeout': 10,\n",
    "                           'retries': 3,\n",
    "                           },\n",
    "             chunksize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9f61001-514e-49d0-b37a-32fdd07a19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4040b24a-a031-4126-9f92-8a7f91d5801d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sumw': defaultdict_accumulator(float,\n",
       "                         {'SUEP-m1000-darkPhoHad+RunIIAutumn18-private+MINIAODSIM': 99.96999740600586}),\n",
       " 'total': defaultdict_accumulator(float,\n",
       "                         {'SUEP-m1000-darkPhoHad+RunIIAutumn18-private+MINIAODSIM': 100.0}),\n",
       " 'cut1': defaultdict_accumulator(float,\n",
       "                         {'SUEP-m1000-darkPhoHad+RunIIAutumn18-private+MINIAODSIM': 6.0}),\n",
       " 'cut2': defaultdict_accumulator(float,\n",
       "                         {'SUEP-m1000-darkPhoHad+RunIIAutumn18-private+MINIAODSIM': 6.0}),\n",
       " 'cut3': defaultdict_accumulator(float,\n",
       "                         {'SUEP-m1000-darkPhoHad+RunIIAutumn18-private+MINIAODSIM': 6.0}),\n",
       " 'cut4': defaultdict_accumulator(float,\n",
       "                         {'SUEP-m1000-darkPhoHad+RunIIAutumn18-private+MINIAODSIM': 6.0}),\n",
       " 'cut5': defaultdict_accumulator(float,\n",
       "                         {'SUEP-m1000-darkPhoHad+RunIIAutumn18-private+MINIAODSIM': 6.0})}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07d88d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'defaultdict_accumulator' object has no attribute 'scale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msumw\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(key)\n\u001b[0;32m---> 17\u001b[0m     \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m(scales, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m coffea\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39msave(output, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.coffea\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'defaultdict_accumulator' object has no attribute 'scale'"
     ]
    }
   ],
   "source": [
    "# just in case this isn't working as expected\n",
    "coffea.util.save(output, \"unscaled_output.coffea\")\n",
    "\n",
    "# calculate normalization\n",
    "scales = {} \n",
    "for dataset in output[\"sumw\"]:\n",
    "    xsec = xsections[dataset]\n",
    "    scale = xsec / output[\"sumw\"][dataset]\n",
    "    scales.update({dataset: scale})\n",
    "\n",
    "# apply normalization to all histograms\n",
    "for key in list(output.keys()):\n",
    "    if key.lower() == 'sumw': continue\n",
    "    print(key)\n",
    "    output[key].scale(scales, axis='dataset')\n",
    "\n",
    "coffea.util.save(output, \"output.coffea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7aa92",
   "metadata": {},
   "source": [
    "### We can make some plots here too! But most of the analysis is in Dask_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce1517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb878ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "suep",
   "language": "python",
   "name": "suep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
